[
    {
        "id": "c058be4574f2871a3230875661861fdc",
        "paper": {
            "title": "Human-AI Collaborative Uncertainty Quantification",
            "summary": "AI predictive systems are increasingly embedded in decision making pipelines,\nshaping high stakes choices once made solely by humans. Yet robust decisions\nunder uncertainty still rely on capabilities that current AI lacks: domain\nknowledge not captured by data, long horizon context, and reasoning grounded in\nthe physical world. This gap has motivated growing efforts to design\ncollaborative frameworks that combine the complementary strengths of humans and\nAI. This work advances this vision by identifying the fundamental principles of\nHuman AI collaboration within uncertainty quantification, a key component of\nreliable decision making. We introduce Human AI Collaborative Uncertainty\nQuantification, a framework that formalizes how an AI model can refine a human\nexpert's proposed prediction set with two goals: avoiding counterfactual harm,\nensuring the AI does not degrade correct human judgments, and complementarity,\nenabling recovery of correct outcomes the human missed. At the population\nlevel, we show that the optimal collaborative prediction set follows an\nintuitive two threshold structure over a single score function, extending a\nclassical result in conformal prediction. Building on this insight, we develop\npractical offline and online calibration algorithms with provable distribution\nfree finite sample guarantees. The online method adapts to distribution shifts,\nincluding human behavior evolving through interaction with AI, a phenomenon we\ncall Human to AI Adaptation. Experiments across image classification,\nregression, and text based medical decision making show that collaborative\nprediction sets consistently outperform either agent alone, achieving higher\ncoverage and smaller set sizes across various conditions.",
            "pdf_link": "http://arxiv.org/pdf/2510.23476v1.pdf",
            "query": "fog ai"
        }
    },
    {
        "id": "e33f37a847a7b728f37bb4e4fba5adc9",
        "paper": {
            "title": "Shareholder Democracy with AI Representatives",
            "summary": "A large share of retail investors hold public equities through mutual funds,\nyet lack adequate control over these investments. Indeed, mutual funds\nconcentrate voting power in the hands of a few asset managers. These managers\nvote on behalf of shareholders despite having limited insight into their\nindividual preferences, leaving them exposed to growing political and\nregulatory pressures, particularly amid rising shareholder activism.\nPass-through voting has been proposed as a way to empower retail investors and\nprovide asset managers with clearer guidance, but it faces challenges such as\nlow participation rates and the difficulty of capturing highly individualized\nshareholder preferences for each specific vote. Randomly selected assemblies of\nshareholders, or ``investor assemblies,'' have also been proposed as more\nrepresentative proxies than asset managers. As a third alternative, we propose\nartificial intelligence (AI) enabled representatives trained on individual\nshareholder preferences to act as proxies and vote on their behalf. Over time,\nthese models could not only predict how retail investors would vote at any\ngiven moment but also how they might vote if they had significantly more time,\nknowledge, and resources to evaluate each proposal, leading to better overall\ndecision-making. We argue that shareholder democracy offers a compelling\nreal-world test bed for AI-enabled representation, providing valuable insights\ninto both the potential benefits and risks of this approach more generally.",
            "pdf_link": "http://arxiv.org/pdf/2510.23475v1.pdf",
            "query": "fog ai"
        }
    },
    {
        "id": "d91aa33a89a2dac772eccee515c4c8f6",
        "paper": {
            "title": "Policy-Aware Generative AI for Safe, Auditable Data Access Governance",
            "summary": "Enterprises need access decisions that satisfy least privilege, comply with\nregulations, and remain auditable. We present a policy aware controller that\nuses a large language model (LLM) to interpret natural language requests\nagainst written policies and metadata, not raw data. The system, implemented\nwith Google Gemini~2.0 Flash, executes a six-stage reasoning framework (context\ninterpretation, user validation, data classification, business purpose test,\ncompliance mapping, and risk synthesis) with early hard policy gates and deny\nby default. It returns APPROVE, DENY, CONDITIONAL together with cited controls\nand a machine readable rationale. We evaluate on fourteen canonical cases\nacross seven scenario families using a privacy preserving benchmark. Results\nshow Exact Decision Match improving from 10/14 to 13/14 (92.9\\%) after applying\npolicy gates, DENY recall rising to 1.00, False Approval Rate on must-deny\nfamilies dropping to 0, and Functional Appropriateness and Compliance Adherence\nat 14/14. Expert ratings of rationale quality are high, and median latency is\nunder one minute. These findings indicate that policy constrained LLM\nreasoning, combined with explicit gates and audit trails, can translate human\nreadable policies into safe, compliant, and traceable machine decisions.",
            "pdf_link": "http://arxiv.org/pdf/2510.23474v1.pdf",
            "query": "fog ai"
        }
    },
    {
        "id": "4381759471dca9a640955ba4ea61d536",
        "paper": {
            "title": "Detecting the Use of Generative AI in Crowdsourced Surveys: Implications for Data Integrity",
            "summary": "The widespread adoption of generative AI (GenAI) has introduced new\nchallenges in crowdsourced data collection, particularly in survey-based\nresearch. While GenAI offers powerful capabilities, its unintended use in\ncrowdsourcing, such as generating automated survey responses, threatens the\nintegrity of empirical research and complicates efforts to understand public\nopinion and behavior. In this study, we investigate and evaluate two approaches\nfor detecting AI-generated responses in online surveys: LLM-based detection and\nsignature-based detection. We conducted experiments across seven survey\nstudies, comparing responses collected before 2022 with those collected after\nthe release of ChatGPT. Our findings reveal a significant increase in\nAI-generated responses in the post-2022 studies, highlighting how GenAI may\nsilently distort crowdsourced data. This work raises broader concerns about\nevolving landscape of data integrity, where GenAI can compromise data quality,\nmislead researchers, and influence downstream findings in fields such as\nhealth, politics, and social behavior. By surfacing detection strategies and\nempirical evidence of GenAI's impact, we aim to contribute to ongoing\nconversation about safeguarding research integrity and supporting scholars\nnavigating these methodological and ethical challenges.",
            "pdf_link": "http://arxiv.org/pdf/2510.24594v1.pdf",
            "query": "fog ai"
        }
    },
    {
        "id": "6fba8ec0359696425542c9deeaf8a34c",
        "paper": {
            "title": "ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?",
            "summary": "Frontier AI agents show increasing promise as scientific research assistants,\nand may eventually be useful for extended, open-ended research workflows.\nHowever, in order to use agents for novel research, we must first assess the\nunderlying faithfulness and correctness of their work. To evaluate agents as\nresearch assistants, we introduce ReplicationBench, an evaluation framework\nthat tests whether agents can replicate entire research papers drawn from the\nastrophysics literature. Astrophysics, where research relies heavily on\narchival data and computational study while requiring little real-world\nexperimentation, is a particularly useful testbed for AI agents in scientific\nresearch. We split each paper into tasks which require agents to replicate the\npaper's core contributions, including the experimental setup, derivations, data\nanalysis, and codebase. Each task is co-developed with the original paper\nauthors and targets a key scientific result, enabling objective evaluation of\nboth faithfulness (adherence to original methods) and correctness (technical\naccuracy of results). ReplicationBench is extremely challenging for current\nfrontier language models: even the best-performing language models score under\n20%. We analyze ReplicationBench trajectories in collaboration with domain\nexperts and find a rich, diverse set of failure modes for agents in scientific\nresearch. ReplicationBench establishes the first benchmark of paper-scale,\nexpert-validated astrophysics research tasks, reveals insights about agent\nperformance generalizable to other domains of data-driven science, and provides\na scalable framework for measuring AI agents' reliability in scientific\nresearch.",
            "pdf_link": "http://arxiv.org/pdf/2510.24591v1.pdf",
            "query": "fog ai"
        }
    },
    {
        "id": "2e28f7665fd1942f7a7e8275d1209ad1",
        "paper": {
            "title": "Politically Speaking: LLMs on Changing International Affairs",
            "summary": "Ask your chatbot to impersonate an expert from Russia and an expert from US\nand query it on Chinese politics. How might the outputs differ? Or, to prepare\nourselves for the worse, how might they converge? Scholars have raised concerns\nLLM based applications can homogenize cultures and flatten perspectives. But\nexactly how much does LLM generated outputs converge despite explicit different\nrole assignment? This study provides empirical evidence to the above question.\nThe critique centres on pretrained models regurgitating ossified political\njargons used in the Western world when speaking about China, Iran, Russian, and\nUS politics, despite changes in these countries happening daily or hourly. The\nexperiments combine role-prompting and similarity metrics. The results show\nthat AI generated discourses from four models about Iran and China are the most\nhomogeneous and unchanging across all four models, including OpenAI GPT, Google\nGemini, Anthropic Claude, and DeepSeek, despite the prompted perspective change\nand the actual changes in real life. This study does not engage with history,\npolitics, or literature as traditional disciplinary approaches would; instead,\nit takes cues from international and area studies and offers insight on the\nfuture trajectory of shifting political discourse in a digital space\nincreasingly cannibalised by AI.",
            "pdf_link": "http://arxiv.org/pdf/2510.24582v1.pdf",
            "query": "fog ai"
        }
    },
    {
        "id": "6d721f2656aa81f693d77aee8ea5eae6",
        "paper": {
            "title": "Communication and Verification in LLM Agents towards Collaboration under Information Asymmetry",
            "summary": "While Large Language Model (LLM) agents are often approached from the angle\nof action planning/generation to accomplish a goal (e.g., given by language\ndescriptions), their abilities to collaborate with each other to achieve a\njoint goal are not well explored. To address this limitation, this paper\nstudies LLM agents in task collaboration, particularly under the condition of\ninformation asymmetry, where agents have disparities in their knowledge and\nskills and need to work together to complete a shared task. We extend Einstein\nPuzzles, a classical symbolic puzzle, to a table-top game. In this game, two\nLLM agents must reason, communicate, and act to satisfy spatial and relational\nconstraints required to solve the puzzle. We apply a fine-tuning-plus-verifier\nframework in which LLM agents are equipped with various communication\nstrategies and verification signals from the environment. Empirical results\nhighlight the critical importance of aligned communication, especially when\nagents possess both information-seeking and -providing capabilities.\nInterestingly, agents without communication can still achieve high task\nperformance; however, further analysis reveals a lack of true rule\nunderstanding and lower trust from human evaluators. Instead, by integrating an\nenvironment-based verifier, we enhance agents' ability to comprehend task rules\nand complete tasks, promoting both safer and more interpretable collaboration\nin AI systems. https://github.com/Roihn/EinsteinPuzzles",
            "pdf_link": "http://arxiv.org/pdf/2510.25595v1.pdf",
            "query": "fog ai"
        }
    },
    {
        "id": "340b23e525a89bb8580a6b19b4de9c70",
        "paper": {
            "title": "Standardization of Psychiatric Diagnoses -- Role of Fine-tuned LLM Consortium and OpenAI-gpt-oss Reasoning LLM Enabled Decision Support System",
            "summary": "The diagnosis of most mental disorders, including psychiatric evaluations,\nprimarily depends on dialogues between psychiatrists and patients. This\nsubjective process can lead to variability in diagnoses across clinicians and\npatients, resulting in inconsistencies and challenges in achieving reliable\noutcomes. To address these issues and standardize psychiatric diagnoses, we\npropose a Fine-Tuned Large Language Model (LLM) Consortium and OpenAI-gpt-oss\nReasoning LLM-enabled Decision Support System for the clinical diagnosis of\nmental disorders. Our approach leverages fine-tuned LLMs trained on\nconversational datasets involving psychiatrist-patient interactions focused on\nmental health conditions (e.g., depression). The diagnostic predictions from\nindividual models are aggregated through a consensus-based decision-making\nprocess, refined by the OpenAI-gpt-oss reasoning LLM. We propose a novel method\nfor deploying LLM agents that orchestrate communication between the LLM\nconsortium and the reasoning LLM, ensuring transparency, reliability, and\nresponsible AI across the entire diagnostic workflow. Experimental results\ndemonstrate the transformative potential of combining fine-tuned LLMs with a\nreasoning model to create a robust and highly accurate diagnostic system for\nmental health assessment. A prototype of the proposed platform, integrating\nthree fine-tuned LLMs with the OpenAI-gpt-oss reasoning LLM, was developed in\ncollaboration with the U.S. Army Medical Research Team in Norfolk, Virginia,\nUSA. To the best of our knowledge, this work represents the first application\nof a fine-tuned LLM consortium integrated with a reasoning LLM for clinical\nmental health diagnosis paving the way for next-generation AI-powered eHealth\nsystems aimed at standardizing psychiatric diagnoses.",
            "pdf_link": "http://arxiv.org/pdf/2510.25588v1.pdf",
            "query": "fog ai"
        }
    }
]