[
    {
        "id": "1c6624ecaf09c770a537a1637ca8f25b",
        "paper": {
            "title": "PREVENT: Proactive Risk Evaluation and Vigilant Execution of Tasks for Mobile Robotic Chemists using Multi-Modal Behavior Trees",
            "summary": "Mobile robotic chemists are a fast growing trend in the field of chemistry\nand materials research. However, so far these mobile robots lack workflow\nawareness skills. This poses the risk that even a small anomaly, such as an\nimproperly capped sample vial could disrupt the entire workflow. This wastes\ntime, and resources, and could pose risks to human researchers, such as\nexposure to toxic materials. Existing perception mechanisms can be used to\npredict anomalies but they often generate excessive false positives. This may\nhalt workflow execution unnecessarily, requiring researchers to intervene and\nto resume the workflow when no problem actually exists, negating the benefits\nof autonomous operation. To address this problem, we propose PREVENT a system\ncomprising navigation and manipulation skills based on a multimodal Behavior\nTree (BT) approach that can be integrated into existing software architectures\nwith minimal modifications. Our approach involves a hierarchical perception\nmechanism that exploits AI techniques and sensory feedback through Dexterous\nVision and Navigational Vision cameras and an IoT gas sensor module for\nexecution-related decision-making. Experimental evaluations show that the\nproposed approach is comparatively efficient and completely avoids both false\nnegatives and false positives when tested in simulated risk scenarios within\nour robotic chemistry workflow. The results also show that the proposed\nmulti-modal perception skills achieved deployment accuracies that were higher\nthan the average of the corresponding uni-modal skills, both for navigation and\nfor manipulation.",
            "pdf_link": "http://arxiv.org/pdf/2510.21438v1.pdf",
            "query": "fog ai"
        }
    },
    {
        "id": "c058be4574f2871a3230875661861fdc",
        "paper": {
            "title": "Human-AI Collaborative Uncertainty Quantification",
            "summary": "AI predictive systems are increasingly embedded in decision making pipelines,\nshaping high stakes choices once made solely by humans. Yet robust decisions\nunder uncertainty still rely on capabilities that current AI lacks: domain\nknowledge not captured by data, long horizon context, and reasoning grounded in\nthe physical world. This gap has motivated growing efforts to design\ncollaborative frameworks that combine the complementary strengths of humans and\nAI. This work advances this vision by identifying the fundamental principles of\nHuman AI collaboration within uncertainty quantification, a key component of\nreliable decision making. We introduce Human AI Collaborative Uncertainty\nQuantification, a framework that formalizes how an AI model can refine a human\nexpert's proposed prediction set with two goals: avoiding counterfactual harm,\nensuring the AI does not degrade correct human judgments, and complementarity,\nenabling recovery of correct outcomes the human missed. At the population\nlevel, we show that the optimal collaborative prediction set follows an\nintuitive two threshold structure over a single score function, extending a\nclassical result in conformal prediction. Building on this insight, we develop\npractical offline and online calibration algorithms with provable distribution\nfree finite sample guarantees. The online method adapts to distribution shifts,\nincluding human behavior evolving through interaction with AI, a phenomenon we\ncall Human to AI Adaptation. Experiments across image classification,\nregression, and text based medical decision making show that collaborative\nprediction sets consistently outperform either agent alone, achieving higher\ncoverage and smaller set sizes across various conditions.",
            "pdf_link": "http://arxiv.org/pdf/2510.23476v1.pdf",
            "query": "fog ai"
        }
    },
    {
        "id": "e33f37a847a7b728f37bb4e4fba5adc9",
        "paper": {
            "title": "Shareholder Democracy with AI Representatives",
            "summary": "A large share of retail investors hold public equities through mutual funds,\nyet lack adequate control over these investments. Indeed, mutual funds\nconcentrate voting power in the hands of a few asset managers. These managers\nvote on behalf of shareholders despite having limited insight into their\nindividual preferences, leaving them exposed to growing political and\nregulatory pressures, particularly amid rising shareholder activism.\nPass-through voting has been proposed as a way to empower retail investors and\nprovide asset managers with clearer guidance, but it faces challenges such as\nlow participation rates and the difficulty of capturing highly individualized\nshareholder preferences for each specific vote. Randomly selected assemblies of\nshareholders, or ``investor assemblies,'' have also been proposed as more\nrepresentative proxies than asset managers. As a third alternative, we propose\nartificial intelligence (AI) enabled representatives trained on individual\nshareholder preferences to act as proxies and vote on their behalf. Over time,\nthese models could not only predict how retail investors would vote at any\ngiven moment but also how they might vote if they had significantly more time,\nknowledge, and resources to evaluate each proposal, leading to better overall\ndecision-making. We argue that shareholder democracy offers a compelling\nreal-world test bed for AI-enabled representation, providing valuable insights\ninto both the potential benefits and risks of this approach more generally.",
            "pdf_link": "http://arxiv.org/pdf/2510.23475v1.pdf",
            "query": "fog ai"
        }
    },
    {
        "id": "d91aa33a89a2dac772eccee515c4c8f6",
        "paper": {
            "title": "Policy-Aware Generative AI for Safe, Auditable Data Access Governance",
            "summary": "Enterprises need access decisions that satisfy least privilege, comply with\nregulations, and remain auditable. We present a policy aware controller that\nuses a large language model (LLM) to interpret natural language requests\nagainst written policies and metadata, not raw data. The system, implemented\nwith Google Gemini~2.0 Flash, executes a six-stage reasoning framework (context\ninterpretation, user validation, data classification, business purpose test,\ncompliance mapping, and risk synthesis) with early hard policy gates and deny\nby default. It returns APPROVE, DENY, CONDITIONAL together with cited controls\nand a machine readable rationale. We evaluate on fourteen canonical cases\nacross seven scenario families using a privacy preserving benchmark. Results\nshow Exact Decision Match improving from 10/14 to 13/14 (92.9\\%) after applying\npolicy gates, DENY recall rising to 1.00, False Approval Rate on must-deny\nfamilies dropping to 0, and Functional Appropriateness and Compliance Adherence\nat 14/14. Expert ratings of rationale quality are high, and median latency is\nunder one minute. These findings indicate that policy constrained LLM\nreasoning, combined with explicit gates and audit trails, can translate human\nreadable policies into safe, compliant, and traceable machine decisions.",
            "pdf_link": "http://arxiv.org/pdf/2510.23474v1.pdf",
            "query": "fog ai"
        }
    },
    {
        "id": "4381759471dca9a640955ba4ea61d536",
        "paper": {
            "title": "Detecting the Use of Generative AI in Crowdsourced Surveys: Implications for Data Integrity",
            "summary": "The widespread adoption of generative AI (GenAI) has introduced new\nchallenges in crowdsourced data collection, particularly in survey-based\nresearch. While GenAI offers powerful capabilities, its unintended use in\ncrowdsourcing, such as generating automated survey responses, threatens the\nintegrity of empirical research and complicates efforts to understand public\nopinion and behavior. In this study, we investigate and evaluate two approaches\nfor detecting AI-generated responses in online surveys: LLM-based detection and\nsignature-based detection. We conducted experiments across seven survey\nstudies, comparing responses collected before 2022 with those collected after\nthe release of ChatGPT. Our findings reveal a significant increase in\nAI-generated responses in the post-2022 studies, highlighting how GenAI may\nsilently distort crowdsourced data. This work raises broader concerns about\nevolving landscape of data integrity, where GenAI can compromise data quality,\nmislead researchers, and influence downstream findings in fields such as\nhealth, politics, and social behavior. By surfacing detection strategies and\nempirical evidence of GenAI's impact, we aim to contribute to ongoing\nconversation about safeguarding research integrity and supporting scholars\nnavigating these methodological and ethical challenges.",
            "pdf_link": "http://arxiv.org/pdf/2510.24594v1.pdf",
            "query": "fog ai"
        }
    },
    {
        "id": "6fba8ec0359696425542c9deeaf8a34c",
        "paper": {
            "title": "ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?",
            "summary": "Frontier AI agents show increasing promise as scientific research assistants,\nand may eventually be useful for extended, open-ended research workflows.\nHowever, in order to use agents for novel research, we must first assess the\nunderlying faithfulness and correctness of their work. To evaluate agents as\nresearch assistants, we introduce ReplicationBench, an evaluation framework\nthat tests whether agents can replicate entire research papers drawn from the\nastrophysics literature. Astrophysics, where research relies heavily on\narchival data and computational study while requiring little real-world\nexperimentation, is a particularly useful testbed for AI agents in scientific\nresearch. We split each paper into tasks which require agents to replicate the\npaper's core contributions, including the experimental setup, derivations, data\nanalysis, and codebase. Each task is co-developed with the original paper\nauthors and targets a key scientific result, enabling objective evaluation of\nboth faithfulness (adherence to original methods) and correctness (technical\naccuracy of results). ReplicationBench is extremely challenging for current\nfrontier language models: even the best-performing language models score under\n20%. We analyze ReplicationBench trajectories in collaboration with domain\nexperts and find a rich, diverse set of failure modes for agents in scientific\nresearch. ReplicationBench establishes the first benchmark of paper-scale,\nexpert-validated astrophysics research tasks, reveals insights about agent\nperformance generalizable to other domains of data-driven science, and provides\na scalable framework for measuring AI agents' reliability in scientific\nresearch.",
            "pdf_link": "http://arxiv.org/pdf/2510.24591v1.pdf",
            "query": "fog ai"
        }
    },
    {
        "id": "2e28f7665fd1942f7a7e8275d1209ad1",
        "paper": {
            "title": "Politically Speaking: LLMs on Changing International Affairs",
            "summary": "Ask your chatbot to impersonate an expert from Russia and an expert from US\nand query it on Chinese politics. How might the outputs differ? Or, to prepare\nourselves for the worse, how might they converge? Scholars have raised concerns\nLLM based applications can homogenize cultures and flatten perspectives. But\nexactly how much does LLM generated outputs converge despite explicit different\nrole assignment? This study provides empirical evidence to the above question.\nThe critique centres on pretrained models regurgitating ossified political\njargons used in the Western world when speaking about China, Iran, Russian, and\nUS politics, despite changes in these countries happening daily or hourly. The\nexperiments combine role-prompting and similarity metrics. The results show\nthat AI generated discourses from four models about Iran and China are the most\nhomogeneous and unchanging across all four models, including OpenAI GPT, Google\nGemini, Anthropic Claude, and DeepSeek, despite the prompted perspective change\nand the actual changes in real life. This study does not engage with history,\npolitics, or literature as traditional disciplinary approaches would; instead,\nit takes cues from international and area studies and offers insight on the\nfuture trajectory of shifting political discourse in a digital space\nincreasingly cannibalised by AI.",
            "pdf_link": "http://arxiv.org/pdf/2510.24582v1.pdf",
            "query": "fog ai"
        }
    }
]